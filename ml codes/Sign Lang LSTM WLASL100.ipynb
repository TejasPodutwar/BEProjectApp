{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55287d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f418fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdfde2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbde47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # human pose, face landmarks, and hand tracking \n",
    "mp_drawing = mp.solutions.drawing_utils # To draw the keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bdef158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # By deafult captured image isn't rgb\n",
    "    image.flags.writeable = False # Can't edit during processing\n",
    "    results = model.process(image)\n",
    "    image.flags.writeable = True \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7d5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,      \n",
    "    mp_drawing.DrawingSpec(\n",
    "        color=(255,0,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "      ),\n",
    "      mp_drawing.DrawingSpec(\n",
    "        color=(0,255,255),\n",
    "        thickness=1,\n",
    "        circle_radius=1\n",
    "      ))\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab01767",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"WLASL_v0.3.json\") as f:\n",
    "    data = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b093cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_ids(word):\n",
    "    results = []\n",
    "    for i in data:\n",
    "        if(i['gloss'] == word):\n",
    "            for j in i['instances']:\n",
    "                results.append(j['video_id'])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97517643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first 100 words\n",
    "cnt = 0\n",
    "words = []\n",
    "for i in data:\n",
    "    if(cnt==100):\n",
    "        break\n",
    "    words.append(i['gloss'])\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a1a7f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Finding the minimum frames in a video\n",
    "min_frame_count = sys.maxsize\n",
    "min_count = 1000000000\n",
    "video_num = 1\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for word in words:\n",
    "        # print(f\"Processing video no. {video_num}\")\n",
    "        videos = get_video_ids(word)\n",
    "        # Loop through sequences aka videos\n",
    "        count = 0\n",
    "        for vid_id in videos:\n",
    "            # Loop through video length \n",
    "            if(os.path.isfile(f\"dataset/videos/{vid_id}.mp4\")):\n",
    "                count+= 1\n",
    "                cap = cv2.VideoCapture(f\"dataset/videos/{vid_id}.mp4\")\n",
    "                # no. of frames in a video\n",
    "                length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "                if(length < min_frame_count):\n",
    "                    min_frame_count = length\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "        if(count < min_count):\n",
    "            min_count = count\n",
    "        video_num+=1\n",
    "print(min_frame_count)\n",
    "print(min_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b36644d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum no. of frames:  19\n"
     ]
    }
   ],
   "source": [
    "video_dir = './dataset/videos'\n",
    "\n",
    "# Data map to store the video ids of the videos of the words present in the dataset\n",
    "data_mp = dict()\n",
    "for word in words:\n",
    "    data_mp[word] = []\n",
    "\n",
    "# Store the min frames count\n",
    "min_frames_count = 1000000\n",
    "\n",
    "for word in words:\n",
    "\n",
    "    # Get video ids for of all the videos of a word\n",
    "    video_ids = get_video_ids(word)\n",
    "\n",
    "    for video_id in video_ids:\n",
    "        videoPath = f\"{video_dir}/{video_id}.mp4\"\n",
    "        video_reader = cv2.VideoCapture(videoPath)\n",
    "        count  = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        if (count!=0):\n",
    "            data_mp[word].append(video_id)\n",
    "            min_frames_count  = min(min_frames_count,count)\n",
    "\n",
    "print(\"Minimum no. of frames: \",min_frames_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a23e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "#     return np.concatenate([pose, face, lh, rh])\n",
    "    return np.concatenate([pose, lh, rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1590e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    for word in words:\n",
    "        videos = data_mp[word]\n",
    "        cnt_word = 1\n",
    "        # Loop through sequences aka videos\n",
    "        for vid_id in videos:\n",
    "            \n",
    "            # Loop through video length aka sequence length\n",
    "            cap = cv2.VideoCapture(f\"dataset/videos/{vid_id}.mp4\")\n",
    "            count = 0\n",
    "            cur_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            curr_skip_len = cur_len//min_frame_count\n",
    "            fram_no = 0\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if(not(ret)):\n",
    "                    break\n",
    "                # Make detections\n",
    "                image, results = mp_detection(frame, holistic)\n",
    "                # print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                \n",
    "                cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                if(not(os.path.isdir(os.path.join(\"Data_without_face\", word, vid_id)))):\n",
    "                    os.makedirs(os.path.join(\"Data_without_face\", word, vid_id))\n",
    "                #file = open(os.path.join(\"Data\", word, vid_id, str(count)+\".npy\"), 'x')\n",
    "                #file.close()\n",
    "                npy_path = os.path.join(\"Data_without_face\", word, vid_id, str(count))\n",
    "                \n",
    "\n",
    "                if(not(os.path.isfile(npy_path))):\n",
    "                    np.save(npy_path, keypoints)\n",
    "                fram_no += curr_skip_len\n",
    "                count+=1\n",
    "                #print(count, min_frame_count)\n",
    "                if(count+1 >= min_frame_count):\n",
    "                    break\n",
    "                currentPos = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "                #print(\"Current\", currentPos)\n",
    "                #print(\"Supposed\", fram_no)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, fram_no)\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            #print(\"Count\", count)\n",
    "            #if(os.path.isdir(os.path.join(\"Data\", word, vid_id))):\n",
    "                #print(len(os.listdir(os.path.join(\"Data\", word, vid_id))))\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee50bf5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video no. 1 processing..\n",
      "Video no. 2 processing..\n",
      "Video no. 3 processing..\n",
      "Video no. 4 processing..\n",
      "Video no. 5 processing..\n",
      "Video no. 6 processing..\n",
      "Video no. 7 processing..\n",
      "Video no. 8 processing..\n",
      "Video no. 9 processing..\n",
      "Video no. 10 processing..\n",
      "Video no. 11 processing..\n",
      "Video no. 12 processing..\n",
      "Video no. 13 processing..\n",
      "Video no. 14 processing..\n",
      "Video no. 15 processing..\n",
      "Video no. 16 processing..\n",
      "Video no. 17 processing..\n",
      "Video no. 18 processing..\n",
      "Video no. 19 processing..\n",
      "Video no. 20 processing..\n",
      "Video no. 21 processing..\n",
      "Video no. 22 processing..\n",
      "Video no. 23 processing..\n",
      "Video no. 24 processing..\n",
      "Video no. 25 processing..\n",
      "Video no. 26 processing..\n",
      "Video no. 27 processing..\n",
      "Video no. 28 processing..\n",
      "Video no. 29 processing..\n",
      "Video no. 30 processing..\n",
      "Video no. 31 processing..\n",
      "Video no. 32 processing..\n",
      "Video no. 33 processing..\n",
      "Video no. 34 processing..\n",
      "Video no. 35 processing..\n",
      "Video no. 36 processing..\n",
      "Video no. 37 processing..\n",
      "Video no. 38 processing..\n",
      "Video no. 39 processing..\n",
      "Video no. 40 processing..\n",
      "Video no. 41 processing..\n",
      "Video no. 42 processing..\n",
      "Video no. 43 processing..\n",
      "Video no. 44 processing..\n",
      "Video no. 45 processing..\n",
      "Video no. 46 processing..\n",
      "Video no. 47 processing..\n",
      "Video no. 48 processing..\n",
      "Video no. 49 processing..\n",
      "Video no. 50 processing..\n",
      "Video no. 51 processing..\n",
      "Video no. 52 processing..\n",
      "Video no. 53 processing..\n",
      "Video no. 54 processing..\n",
      "Video no. 55 processing..\n",
      "Video no. 56 processing..\n",
      "Video no. 57 processing..\n",
      "Video no. 58 processing..\n",
      "Video no. 59 processing..\n",
      "Video no. 60 processing..\n",
      "Video no. 61 processing..\n",
      "Video no. 62 processing..\n",
      "Video no. 63 processing..\n",
      "Video no. 64 processing..\n",
      "Video no. 65 processing..\n",
      "Video no. 66 processing..\n",
      "Video no. 67 processing..\n",
      "Video no. 68 processing..\n",
      "Video no. 69 processing..\n",
      "Video no. 70 processing..\n",
      "Video no. 71 processing..\n",
      "Video no. 72 processing..\n",
      "Video no. 73 processing..\n",
      "Video no. 74 processing..\n",
      "Video no. 75 processing..\n",
      "Video no. 76 processing..\n",
      "Video no. 77 processing..\n",
      "Video no. 78 processing..\n",
      "Video no. 79 processing..\n",
      "Video no. 80 processing..\n",
      "Video no. 81 processing..\n",
      "Video no. 82 processing..\n",
      "Video no. 83 processing..\n",
      "Video no. 84 processing..\n",
      "Video no. 85 processing..\n",
      "Video no. 86 processing..\n",
      "Video no. 87 processing..\n",
      "Video no. 88 processing..\n",
      "Video no. 89 processing..\n",
      "Video no. 90 processing..\n",
      "Video no. 91 processing..\n",
      "Video no. 92 processing..\n",
      "Video no. 93 processing..\n",
      "Video no. 94 processing..\n",
      "Video no. 95 processing..\n",
      "Video no. 96 processing..\n",
      "Video no. 97 processing..\n",
      "Video no. 98 processing..\n",
      "Video no. 99 processing..\n",
      "Video no. 100 processing..\n"
     ]
    }
   ],
   "source": [
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    # NEW LOOP\n",
    "    # Loop through actions\n",
    "    cnt_word = 1\n",
    "    \n",
    "    for word in words:\n",
    "        videos = data_mp[word]\n",
    "        # Loop through sequences aka videos\n",
    "        \n",
    "        print(f\"Video no. {cnt_word} processing..\")\n",
    "        for vid_id in videos:\n",
    "            \n",
    "            # Loop through video length aka sequence length\n",
    "            cap = cv2.VideoCapture(f\"dataset/videos/{vid_id}.mp4\")\n",
    "            count = 0\n",
    "            cur_len = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            curr_skip_len = cur_len//min_frame_count\n",
    "            fram_no = 0\n",
    "            \n",
    "            while cap.isOpened():\n",
    "                # Read feed\n",
    "                ret, frame = cap.read()\n",
    "                if(not(ret)):\n",
    "                    break\n",
    "                # Make detections\n",
    "                image, results = mp_detection(frame, holistic)\n",
    "                # print(results)\n",
    "\n",
    "                # Draw landmarks\n",
    "                draw_landmarks(image, results)\n",
    "                \n",
    "                cv2.imshow(\"OpenCV Feed\", image)\n",
    "\n",
    "                # NEW Export keypoints\n",
    "                keypoints = extract_keypoints(results)\n",
    "                if(not(os.path.isdir(os.path.join(\"Data_with_face\", word, vid_id)))):\n",
    "                    os.makedirs(os.path.join(\"Data_with_face\", word, vid_id))\n",
    "                #file = open(os.path.join(\"Data\", word, vid_id, str(count)+\".npy\"), 'x')\n",
    "                #file.close()\n",
    "                npy_path = os.path.join(\"Data_with_face\", word, vid_id, str(count))\n",
    "                \n",
    "\n",
    "                if(not(os.path.isfile(npy_path))):\n",
    "                    np.save(npy_path, keypoints)\n",
    "                fram_no += curr_skip_len\n",
    "                count+=1\n",
    "                #print(count, min_frame_count)\n",
    "                if(count+1 >= min_frame_count):\n",
    "                    break\n",
    "                currentPos = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "                #print(\"Current\", currentPos)\n",
    "                #print(\"Supposed\", fram_no)\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, fram_no)\n",
    "                # Break gracefully\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "            #print(\"Count\", count)\n",
    "            #if(os.path.isdir(os.path.join(\"Data\", word, vid_id))):\n",
    "                #print(len(os.listdir(os.path.join(\"Data\", word, vid_id))))\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "        cnt_word +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90151e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a89a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0bd6249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'book': 0,\n",
       " 'drink': 1,\n",
       " 'computer': 2,\n",
       " 'before': 3,\n",
       " 'chair': 4,\n",
       " 'go': 5,\n",
       " 'clothes': 6,\n",
       " 'who': 7,\n",
       " 'candy': 8,\n",
       " 'cousin': 9,\n",
       " 'deaf': 10,\n",
       " 'fine': 11,\n",
       " 'help': 12,\n",
       " 'no': 13,\n",
       " 'thin': 14,\n",
       " 'walk': 15,\n",
       " 'year': 16,\n",
       " 'yes': 17,\n",
       " 'all': 18,\n",
       " 'black': 19,\n",
       " 'cool': 20,\n",
       " 'finish': 21,\n",
       " 'hot': 22,\n",
       " 'like': 23,\n",
       " 'many': 24,\n",
       " 'mother': 25,\n",
       " 'now': 26,\n",
       " 'orange': 27,\n",
       " 'table': 28,\n",
       " 'thanksgiving': 29,\n",
       " 'what': 30,\n",
       " 'woman': 31,\n",
       " 'bed': 32,\n",
       " 'blue': 33,\n",
       " 'bowling': 34,\n",
       " 'can': 35,\n",
       " 'dog': 36,\n",
       " 'family': 37,\n",
       " 'fish': 38,\n",
       " 'graduate': 39,\n",
       " 'hat': 40,\n",
       " 'hearing': 41,\n",
       " 'kiss': 42,\n",
       " 'language': 43,\n",
       " 'later': 44,\n",
       " 'man': 45,\n",
       " 'shirt': 46,\n",
       " 'study': 47,\n",
       " 'tall': 48,\n",
       " 'white': 49,\n",
       " 'wrong': 50,\n",
       " 'accident': 51,\n",
       " 'apple': 52,\n",
       " 'bird': 53,\n",
       " 'change': 54,\n",
       " 'color': 55,\n",
       " 'corn': 56,\n",
       " 'cow': 57,\n",
       " 'dance': 58,\n",
       " 'dark': 59,\n",
       " 'doctor': 60,\n",
       " 'eat': 61,\n",
       " 'enjoy': 62,\n",
       " 'forget': 63,\n",
       " 'give': 64,\n",
       " 'last': 65,\n",
       " 'meet': 66,\n",
       " 'pink': 67,\n",
       " 'pizza': 68,\n",
       " 'play': 69,\n",
       " 'school': 70,\n",
       " 'secretary': 71,\n",
       " 'short': 72,\n",
       " 'time': 73,\n",
       " 'want': 74,\n",
       " 'work': 75,\n",
       " 'africa': 76,\n",
       " 'basketball': 77,\n",
       " 'birthday': 78,\n",
       " 'brown': 79,\n",
       " 'but': 80,\n",
       " 'cheat': 81,\n",
       " 'city': 82,\n",
       " 'cook': 83,\n",
       " 'decide': 84,\n",
       " 'full': 85,\n",
       " 'how': 86,\n",
       " 'jacket': 87,\n",
       " 'letter': 88,\n",
       " 'medicine': 89,\n",
       " 'need': 90,\n",
       " 'paint': 91,\n",
       " 'paper': 92,\n",
       " 'pull': 93,\n",
       " 'purple': 94,\n",
       " 'right': 95,\n",
       " 'same': 96,\n",
       " 'son': 97,\n",
       " 'tell': 98,\n",
       " 'thursday': 99}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d71f04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word No. 1 processing...\n",
      "Word No. 2 processing...\n",
      "Word No. 3 processing...\n",
      "Word No. 4 processing...\n",
      "Word No. 5 processing...\n",
      "Word No. 6 processing...\n",
      "Word No. 7 processing...\n",
      "Word No. 8 processing...\n",
      "Word No. 9 processing...\n",
      "Word No. 10 processing...\n",
      "Word No. 11 processing...\n",
      "Word No. 12 processing...\n",
      "Word No. 13 processing...\n",
      "Word No. 14 processing...\n",
      "Word No. 15 processing...\n",
      "Word No. 16 processing...\n",
      "Word No. 17 processing...\n",
      "Word No. 18 processing...\n",
      "Word No. 19 processing...\n",
      "Word No. 20 processing...\n",
      "Word No. 21 processing...\n",
      "Word No. 22 processing...\n",
      "Word No. 23 processing...\n",
      "Word No. 24 processing...\n",
      "Word No. 25 processing...\n",
      "Word No. 26 processing...\n",
      "Word No. 27 processing...\n",
      "Word No. 28 processing...\n",
      "Word No. 29 processing...\n",
      "Word No. 30 processing...\n",
      "Word No. 31 processing...\n",
      "Word No. 32 processing...\n",
      "Word No. 33 processing...\n",
      "Word No. 34 processing...\n",
      "Word No. 35 processing...\n",
      "Word No. 36 processing...\n",
      "Word No. 37 processing...\n",
      "Word No. 38 processing...\n",
      "Word No. 39 processing...\n",
      "Word No. 40 processing...\n",
      "Word No. 41 processing...\n",
      "Word No. 42 processing...\n",
      "Word No. 43 processing...\n",
      "Word No. 44 processing...\n",
      "Word No. 45 processing...\n",
      "Word No. 46 processing...\n",
      "Word No. 47 processing...\n",
      "Word No. 48 processing...\n",
      "Word No. 49 processing...\n",
      "Word No. 50 processing...\n",
      "Word No. 51 processing...\n",
      "Word No. 52 processing...\n",
      "Word No. 53 processing...\n",
      "Word No. 54 processing...\n",
      "Word No. 55 processing...\n",
      "Word No. 56 processing...\n",
      "Word No. 57 processing...\n",
      "Word No. 58 processing...\n",
      "Word No. 59 processing...\n",
      "Word No. 60 processing...\n",
      "Word No. 61 processing...\n",
      "Word No. 62 processing...\n",
      "Word No. 63 processing...\n",
      "Word No. 64 processing...\n",
      "Word No. 65 processing...\n",
      "Word No. 66 processing...\n",
      "Word No. 67 processing...\n",
      "Word No. 68 processing...\n",
      "Word No. 69 processing...\n",
      "Word No. 70 processing...\n",
      "Word No. 71 processing...\n",
      "Word No. 72 processing...\n",
      "Word No. 73 processing...\n",
      "Word No. 74 processing...\n",
      "Word No. 75 processing...\n",
      "Word No. 76 processing...\n",
      "Word No. 77 processing...\n",
      "Word No. 78 processing...\n",
      "Word No. 79 processing...\n",
      "Word No. 80 processing...\n",
      "Word No. 81 processing...\n",
      "Word No. 82 processing...\n",
      "Word No. 83 processing...\n",
      "Word No. 84 processing...\n",
      "Word No. 85 processing...\n",
      "Word No. 86 processing...\n",
      "Word No. 87 processing...\n",
      "Word No. 88 processing...\n",
      "Word No. 89 processing...\n",
      "Word No. 90 processing...\n",
      "Word No. 91 processing...\n",
      "Word No. 92 processing...\n",
      "Word No. 93 processing...\n",
      "Word No. 94 processing...\n",
      "Word No. 95 processing...\n",
      "Word No. 96 processing...\n",
      "Word No. 97 processing...\n",
      "Word No. 98 processing...\n",
      "Word No. 99 processing...\n",
      "Word No. 100 processing...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sequences, labels = [], []\n",
    "cnt = 1\n",
    "for word in words:\n",
    "    print(f\"Word No. {cnt} processing...\")\n",
    "    videos = get_video_ids(word)\n",
    "    # Loop through sequences aka videos\n",
    "    for vid_id in videos:\n",
    "        window = []\n",
    "        if(os.path.isdir(os.path.join(\"Data_without_face\", word, vid_id))):\n",
    "#             if(len(os.listdir(os.path.join(\"Data_without_face\", word, vid_id))) + 1 <= 29):\n",
    "#                 #print(vid_id)\n",
    "            for frame_num in range(len(os.listdir(os.path.join(\"Data_without_face\", word, vid_id)))):\n",
    "                res = np.load(os.path.join(\"Data_without_face\", word, vid_id, \"{}.npy\").format(frame_num))\n",
    "                window.append(res)\n",
    "                #print(len(res))\n",
    "            sequences.append(window)\n",
    "            #print(len(window))\n",
    "            labels.append(label_map[word])\n",
    "\n",
    "    # data augmentation        \n",
    "    for vid_id in videos:\n",
    "        window = []\n",
    "        if(os.path.isdir(os.path.join(\"Data_without_face\", word, vid_id))):\n",
    "#             if(len(os.listdir(os.path.join(\"Data_without_face\", word, vid_id))) + 1 <= 29):\n",
    "#                 #print(vid_id)\n",
    "            for frame_num in range(len(os.listdir(os.path.join(\"Data_without_face\", word, vid_id)))):\n",
    "                res = np.load(os.path.join(\"Data_without_face\", word, vid_id, \"{}.npy\").format(frame_num))\n",
    "                res/= 1.5\n",
    "                window.append(res)\n",
    "                 #print(len(res))\n",
    "            sequences.append(window)\n",
    "            #print(len(window))\n",
    "            labels.append(label_map[word])\n",
    "    \n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2354e06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2026, 18, 258)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3288df1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2026, 18, 258)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= np.asarray(sequences)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf64805c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(labels).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ae266f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "118d9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd9d53a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout, Bidirectional, Embedding\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir = os.path.join('Logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(64, return_sequences=True, activation='relu', input_shape=(18,258)))\n",
    "model.add(GRU(128, return_sequences=True, activation='relu'))\n",
    "model.add(GRU(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(words.shape[0], activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a86525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "456c3ff2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "54/54 [==============================] - 10s 47ms/step - loss: 4.6015 - categorical_accuracy: 0.0122\n",
      "Epoch 2/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 4.4928 - categorical_accuracy: 0.0203\n",
      "Epoch 3/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 4.3081 - categorical_accuracy: 0.0232\n",
      "Epoch 4/2000\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 4.2032 - categorical_accuracy: 0.0250\n",
      "Epoch 5/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 4.1567 - categorical_accuracy: 0.0290\n",
      "Epoch 6/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 4.1143 - categorical_accuracy: 0.0360\n",
      "Epoch 7/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 4.0975 - categorical_accuracy: 0.0279\n",
      "Epoch 8/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 4.0641 - categorical_accuracy: 0.0407\n",
      "Epoch 9/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 4.0683 - categorical_accuracy: 0.0331\n",
      "Epoch 10/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 4.0224 - categorical_accuracy: 0.0337\n",
      "Epoch 11/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 4.0091 - categorical_accuracy: 0.0412\n",
      "Epoch 12/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 3.9595 - categorical_accuracy: 0.0476\n",
      "Epoch 13/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 3.9302 - categorical_accuracy: 0.0447\n",
      "Epoch 14/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 3.8976 - categorical_accuracy: 0.0412\n",
      "Epoch 15/2000\n",
      "54/54 [==============================] - 3s 46ms/step - loss: 3.8725 - categorical_accuracy: 0.0412\n",
      "Epoch 16/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 3.8331 - categorical_accuracy: 0.0563\n",
      "Epoch 17/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 3.7909 - categorical_accuracy: 0.0569\n",
      "Epoch 18/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 3.7841 - categorical_accuracy: 0.0511\n",
      "Epoch 19/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 3.7318 - categorical_accuracy: 0.0627\n",
      "Epoch 20/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 3.7007 - categorical_accuracy: 0.0645\n",
      "Epoch 21/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 3.6469 - categorical_accuracy: 0.0767\n",
      "Epoch 22/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 3.6275 - categorical_accuracy: 0.0755\n",
      "Epoch 23/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 3.5908 - categorical_accuracy: 0.0720\n",
      "Epoch 24/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 3.5445 - categorical_accuracy: 0.0842\n",
      "Epoch 25/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 3.4728 - categorical_accuracy: 0.1010\n",
      "Epoch 26/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 3.4432 - categorical_accuracy: 0.1016\n",
      "Epoch 27/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 3.3331 - categorical_accuracy: 0.1132\n",
      "Epoch 28/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 3.2936 - categorical_accuracy: 0.1301\n",
      "Epoch 29/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 3.2370 - categorical_accuracy: 0.1394\n",
      "Epoch 30/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 3.2242 - categorical_accuracy: 0.1382\n",
      "Epoch 31/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 3.0903 - categorical_accuracy: 0.1527\n",
      "Epoch 32/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 3.0163 - categorical_accuracy: 0.1678\n",
      "Epoch 33/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 2.9567 - categorical_accuracy: 0.1858\n",
      "Epoch 34/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 2.8657 - categorical_accuracy: 0.2009\n",
      "Epoch 35/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 2.7122 - categorical_accuracy: 0.2323\n",
      "Epoch 36/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 2.6338 - categorical_accuracy: 0.2480\n",
      "Epoch 37/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 2.6189 - categorical_accuracy: 0.2544\n",
      "Epoch 38/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 2.5819 - categorical_accuracy: 0.2567\n",
      "Epoch 39/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 2.4734 - categorical_accuracy: 0.2758\n",
      "Epoch 40/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 2.3284 - categorical_accuracy: 0.3107\n",
      "Epoch 41/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 2.2276 - categorical_accuracy: 0.3519\n",
      "Epoch 42/2000\n",
      "54/54 [==============================] - 3s 57ms/step - loss: 2.1939 - categorical_accuracy: 0.3444\n",
      "Epoch 43/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 2.1494 - categorical_accuracy: 0.3606\n",
      "Epoch 44/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 2.0421 - categorical_accuracy: 0.3949\n",
      "Epoch 45/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.9634 - categorical_accuracy: 0.4251\n",
      "Epoch 46/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 1.8908 - categorical_accuracy: 0.4361\n",
      "Epoch 47/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.7579 - categorical_accuracy: 0.4785\n",
      "Epoch 48/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 1.7179 - categorical_accuracy: 0.4890\n",
      "Epoch 49/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 1.6140 - categorical_accuracy: 0.5192\n",
      "Epoch 50/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 1.5646 - categorical_accuracy: 0.5238\n",
      "Epoch 51/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.6610 - categorical_accuracy: 0.4971\n",
      "Epoch 52/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 1.4298 - categorical_accuracy: 0.5662\n",
      "Epoch 53/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.5028 - categorical_accuracy: 0.5424\n",
      "Epoch 54/2000\n",
      "54/54 [==============================] - 3s 53ms/step - loss: 1.3154 - categorical_accuracy: 0.5970\n",
      "Epoch 55/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 1.2774 - categorical_accuracy: 0.6144\n",
      "Epoch 56/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 1.3672 - categorical_accuracy: 0.5807\n",
      "Epoch 57/2000\n",
      "54/54 [==============================] - 3s 54ms/step - loss: 1.4786 - categorical_accuracy: 0.5494\n",
      "Epoch 58/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 1.3166 - categorical_accuracy: 0.5964\n",
      "Epoch 59/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 1.3446 - categorical_accuracy: 0.5714\n",
      "Epoch 60/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 1.1733 - categorical_accuracy: 0.6324\n",
      "Epoch 61/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 1.0963 - categorical_accuracy: 0.6504\n",
      "Epoch 62/2000\n",
      "54/54 [==============================] - 3s 46ms/step - loss: 1.1311 - categorical_accuracy: 0.6394\n",
      "Epoch 63/2000\n",
      "54/54 [==============================] - 2s 46ms/step - loss: 0.9902 - categorical_accuracy: 0.6870\n",
      "Epoch 64/2000\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 0.9050 - categorical_accuracy: 0.7218\n",
      "Epoch 65/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.8531 - categorical_accuracy: 0.7300\n",
      "Epoch 66/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.8298 - categorical_accuracy: 0.7364\n",
      "Epoch 67/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.8339 - categorical_accuracy: 0.7334\n",
      "Epoch 68/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.8409 - categorical_accuracy: 0.7387\n",
      "Epoch 69/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.7699 - categorical_accuracy: 0.7735\n",
      "Epoch 70/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.7485 - categorical_accuracy: 0.7741\n",
      "Epoch 71/2000\n",
      "54/54 [==============================] - 3s 46ms/step - loss: 0.8730 - categorical_accuracy: 0.7218\n",
      "Epoch 72/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.7417 - categorical_accuracy: 0.7695\n",
      "Epoch 73/2000\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 0.6289 - categorical_accuracy: 0.8095\n",
      "Epoch 74/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.6985 - categorical_accuracy: 0.7758\n",
      "Epoch 75/2000\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 0.7379 - categorical_accuracy: 0.7741\n",
      "Epoch 76/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.6354 - categorical_accuracy: 0.8072\n",
      "Epoch 77/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.5775 - categorical_accuracy: 0.8293\n",
      "Epoch 78/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.7489 - categorical_accuracy: 0.7584\n",
      "Epoch 79/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.8293 - categorical_accuracy: 0.7317\n",
      "Epoch 80/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.5508 - categorical_accuracy: 0.8310\n",
      "Epoch 81/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.5047 - categorical_accuracy: 0.8490\n",
      "Epoch 82/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.5350 - categorical_accuracy: 0.8415\n",
      "Epoch 83/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.7318 - categorical_accuracy: 0.7631\n",
      "Epoch 84/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.4790 - categorical_accuracy: 0.8449\n",
      "Epoch 85/2000\n",
      "54/54 [==============================] - 3s 54ms/step - loss: 0.4770 - categorical_accuracy: 0.8566\n",
      "Epoch 86/2000\n",
      "54/54 [==============================] - 3s 53ms/step - loss: 0.4835 - categorical_accuracy: 0.8595\n",
      "Epoch 87/2000\n",
      "54/54 [==============================] - 3s 54ms/step - loss: 0.4985 - categorical_accuracy: 0.8496\n",
      "Epoch 88/2000\n",
      "54/54 [==============================] - 3s 53ms/step - loss: 0.3729 - categorical_accuracy: 0.8931\n",
      "Epoch 89/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.3003 - categorical_accuracy: 0.9100\n",
      "Epoch 90/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.5347 - categorical_accuracy: 0.8322\n",
      "Epoch 91/2000\n",
      "54/54 [==============================] - 3s 53ms/step - loss: 0.5931 - categorical_accuracy: 0.8258\n",
      "Epoch 92/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.6070 - categorical_accuracy: 0.8362\n",
      "Epoch 93/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.3392 - categorical_accuracy: 0.8984\n",
      "Epoch 94/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.2643 - categorical_accuracy: 0.9164\n",
      "Epoch 95/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.2384 - categorical_accuracy: 0.9367\n",
      "Epoch 96/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 1.1200 - categorical_accuracy: 0.7236\n",
      "Epoch 97/2000\n",
      "54/54 [==============================] - 3s 60ms/step - loss: 1.2130 - categorical_accuracy: 0.6533\n",
      "Epoch 98/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.4440 - categorical_accuracy: 0.8780\n",
      "Epoch 99/2000\n",
      "54/54 [==============================] - 3s 54ms/step - loss: 0.3929 - categorical_accuracy: 0.8897\n",
      "Epoch 100/2000\n",
      "54/54 [==============================] - 3s 54ms/step - loss: 0.3847 - categorical_accuracy: 0.8804\n",
      "Epoch 101/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.3194 - categorical_accuracy: 0.9088\n",
      "Epoch 102/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.2145 - categorical_accuracy: 0.9472\n",
      "Epoch 103/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 0.1943 - categorical_accuracy: 0.9466\n",
      "Epoch 104/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 0.2882 - categorical_accuracy: 0.9164\n",
      "Epoch 105/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.2471 - categorical_accuracy: 0.9257\n",
      "Epoch 106/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 0.2186 - categorical_accuracy: 0.9379\n",
      "Epoch 107/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.3096 - categorical_accuracy: 0.9129\n",
      "Epoch 108/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.3318 - categorical_accuracy: 0.8914\n",
      "Epoch 109/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.3533 - categorical_accuracy: 0.8955\n",
      "Epoch 110/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.3178 - categorical_accuracy: 0.9053\n",
      "Epoch 111/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.2297 - categorical_accuracy: 0.9390\n",
      "Epoch 112/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.3590 - categorical_accuracy: 0.8891\n",
      "Epoch 113/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.3690 - categorical_accuracy: 0.8769\n",
      "Epoch 114/2000\n",
      "54/54 [==============================] - 3s 47ms/step - loss: 0.7469 - categorical_accuracy: 0.7915\n",
      "Epoch 115/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.3725 - categorical_accuracy: 0.8839\n",
      "Epoch 116/2000\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 0.2849 - categorical_accuracy: 0.9181\n",
      "Epoch 117/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.5839 - categorical_accuracy: 0.8508\n",
      "Epoch 118/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.4626 - categorical_accuracy: 0.8635\n",
      "Epoch 119/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.1617 - categorical_accuracy: 0.9646\n",
      "Epoch 120/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.1130 - categorical_accuracy: 0.9750\n",
      "Epoch 121/2000\n",
      "54/54 [==============================] - 4s 76ms/step - loss: 0.0947 - categorical_accuracy: 0.9750\n",
      "Epoch 122/2000\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.0849 - categorical_accuracy: 0.9779\n",
      "Epoch 123/2000\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 0.1642 - categorical_accuracy: 0.9564\n",
      "Epoch 124/2000\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 0.2993 - categorical_accuracy: 0.9181\n",
      "Epoch 125/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.1973 - categorical_accuracy: 0.9460\n",
      "Epoch 126/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.1253 - categorical_accuracy: 0.9675\n",
      "Epoch 127/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.2613 - categorical_accuracy: 0.9280\n",
      "Epoch 128/2000\n",
      "54/54 [==============================] - 3s 46ms/step - loss: 0.2230 - categorical_accuracy: 0.9286\n",
      "Epoch 129/2000\n",
      "54/54 [==============================] - 3s 53ms/step - loss: 0.4273 - categorical_accuracy: 0.8780\n",
      "Epoch 130/2000\n",
      "54/54 [==============================] - 3s 62ms/step - loss: 0.2264 - categorical_accuracy: 0.9332\n",
      "Epoch 131/2000\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 0.1510 - categorical_accuracy: 0.9611\n",
      "Epoch 132/2000\n",
      "54/54 [==============================] - 3s 63ms/step - loss: 0.1494 - categorical_accuracy: 0.9652\n",
      "Epoch 133/2000\n",
      "54/54 [==============================] - 3s 58ms/step - loss: 0.3605 - categorical_accuracy: 0.8873\n",
      "Epoch 134/2000\n",
      "54/54 [==============================] - 3s 56ms/step - loss: 0.5389 - categorical_accuracy: 0.8449\n",
      "Epoch 135/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.2756 - categorical_accuracy: 0.9204\n",
      "Epoch 136/2000\n",
      "54/54 [==============================] - 3s 59ms/step - loss: 0.1238 - categorical_accuracy: 0.9669\n",
      "Epoch 137/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.0650 - categorical_accuracy: 0.9866\n",
      "Epoch 138/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.0565 - categorical_accuracy: 0.9895\n",
      "Epoch 139/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.0347 - categorical_accuracy: 0.9954\n",
      "Epoch 140/2000\n",
      "54/54 [==============================] - 3s 49ms/step - loss: 0.0312 - categorical_accuracy: 0.9936\n",
      "Epoch 141/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.0243 - categorical_accuracy: 0.9959\n",
      "Epoch 142/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.0302 - categorical_accuracy: 0.9913\n",
      "Epoch 143/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 3s 50ms/step - loss: 0.0625 - categorical_accuracy: 0.9826\n",
      "Epoch 144/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.3731 - categorical_accuracy: 0.9065\n",
      "Epoch 145/2000\n",
      "54/54 [==============================] - 3s 51ms/step - loss: 0.4439 - categorical_accuracy: 0.8763\n",
      "Epoch 146/2000\n",
      "54/54 [==============================] - 3s 52ms/step - loss: 0.8246 - categorical_accuracy: 0.7660\n",
      "Epoch 147/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.3589 - categorical_accuracy: 0.8972\n",
      "Epoch 148/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.1360 - categorical_accuracy: 0.9704\n",
      "Epoch 149/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.1017 - categorical_accuracy: 0.9779\n",
      "Epoch 150/2000\n",
      "54/54 [==============================] - 3s 50ms/step - loss: 0.2356 - categorical_accuracy: 0.9361\n",
      "Epoch 151/2000\n",
      "54/54 [==============================] - 3s 55ms/step - loss: 0.1828 - categorical_accuracy: 0.9443\n",
      "Epoch 152/2000\n",
      "54/54 [==============================] - 3s 48ms/step - loss: 0.2320 - categorical_accuracy: 0.9413\n",
      "Epoch 153/2000\n",
      "54/54 [==============================] - 2s 38ms/step - loss: 0.2718 - categorical_accuracy: 0.9262\n",
      "Epoch 154/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.1393 - categorical_accuracy: 0.9628\n",
      "Epoch 155/2000\n",
      "54/54 [==============================] - 2s 38ms/step - loss: 0.0770 - categorical_accuracy: 0.9826\n",
      "Epoch 156/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 0.0362 - categorical_accuracy: 0.9936\n",
      "Epoch 157/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.0192 - categorical_accuracy: 0.9983\n",
      "Epoch 158/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0171 - categorical_accuracy: 0.9965\n",
      "Epoch 159/2000\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.0226 - categorical_accuracy: 0.9954\n",
      "Epoch 160/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0520 - categorical_accuracy: 0.9855\n",
      "Epoch 161/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.1854 - categorical_accuracy: 0.9419\n",
      "Epoch 162/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.4507 - categorical_accuracy: 0.8717\n",
      "Epoch 163/2000\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 0.2856 - categorical_accuracy: 0.9170\n",
      "Epoch 164/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.3881 - categorical_accuracy: 0.8815\n",
      "Epoch 165/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.2136 - categorical_accuracy: 0.9344\n",
      "Epoch 166/2000\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.0760 - categorical_accuracy: 0.9814\n",
      "Epoch 167/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 0.0595 - categorical_accuracy: 0.9861\n",
      "Epoch 168/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0262 - categorical_accuracy: 0.9959\n",
      "Epoch 169/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0161 - categorical_accuracy: 0.9983\n",
      "Epoch 170/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0162 - categorical_accuracy: 0.9959\n",
      "Epoch 171/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 0.0120 - categorical_accuracy: 0.9988\n",
      "Epoch 172/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0121 - categorical_accuracy: 0.9983\n",
      "Epoch 173/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0068 - categorical_accuracy: 1.0000\n",
      "Epoch 174/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0056 - categorical_accuracy: 1.0000\n",
      "Epoch 175/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0046 - categorical_accuracy: 1.0000\n",
      "Epoch 176/2000\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.0107 - categorical_accuracy: 0.9988\n",
      "Epoch 177/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0217 - categorical_accuracy: 0.9965\n",
      "Epoch 178/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.6594 - categorical_accuracy: 0.8339\n",
      "Epoch 179/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.6945 - categorical_accuracy: 0.7967\n",
      "Epoch 180/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.2610 - categorical_accuracy: 0.9251\n",
      "Epoch 181/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.3961 - categorical_accuracy: 0.8885\n",
      "Epoch 182/2000\n",
      "54/54 [==============================] - 2s 38ms/step - loss: 0.2351 - categorical_accuracy: 0.9361\n",
      "Epoch 183/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0785 - categorical_accuracy: 0.9837\n",
      "Epoch 184/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0309 - categorical_accuracy: 0.9983\n",
      "Epoch 185/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0175 - categorical_accuracy: 0.9988\n",
      "Epoch 186/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 0.0124 - categorical_accuracy: 0.9988\n",
      "Epoch 187/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.0135 - categorical_accuracy: 0.9983\n",
      "Epoch 188/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0263 - categorical_accuracy: 0.9959\n",
      "Epoch 189/2000\n",
      "54/54 [==============================] - 2s 45ms/step - loss: 0.0161 - categorical_accuracy: 0.9983\n",
      "Epoch 190/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0162 - categorical_accuracy: 0.9983\n",
      "Epoch 191/2000\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.0216 - categorical_accuracy: 0.9948\n",
      "Epoch 192/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.0462 - categorical_accuracy: 0.9878\n",
      "Epoch 193/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.7342 - categorical_accuracy: 0.8252\n",
      "Epoch 194/2000\n",
      "54/54 [==============================] - 2s 40ms/step - loss: 0.6391 - categorical_accuracy: 0.8095\n",
      "Epoch 195/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.3045 - categorical_accuracy: 0.9181\n",
      "Epoch 196/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.1680 - categorical_accuracy: 0.9553\n",
      "Epoch 197/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.1341 - categorical_accuracy: 0.9646\n",
      "Epoch 198/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.1435 - categorical_accuracy: 0.9623\n",
      "Epoch 199/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0657 - categorical_accuracy: 0.9849\n",
      "Epoch 200/2000\n",
      "54/54 [==============================] - 2s 44ms/step - loss: 0.1536 - categorical_accuracy: 0.9588\n",
      "Epoch 201/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.3069 - categorical_accuracy: 0.9059\n",
      "Epoch 202/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.3964 - categorical_accuracy: 0.8873\n",
      "Epoch 203/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.2860 - categorical_accuracy: 0.9094\n",
      "Epoch 204/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.1231 - categorical_accuracy: 0.9657\n",
      "Epoch 205/2000\n",
      "54/54 [==============================] - 2s 43ms/step - loss: 0.0836 - categorical_accuracy: 0.9820\n",
      "Epoch 206/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0266 - categorical_accuracy: 0.9977\n",
      "Epoch 207/2000\n",
      "54/54 [==============================] - 2s 41ms/step - loss: 0.0145 - categorical_accuracy: 1.0000\n",
      "Epoch 208/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0088 - categorical_accuracy: 1.0000\n",
      "Epoch 209/2000\n",
      "54/54 [==============================] - 2s 42ms/step - loss: 0.0063 - categorical_accuracy: 1.0000\n",
      "Epoch 210/2000\n",
      " 2/54 [>.............................] - ETA: 2s - loss: 0.0038 - categorical_accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtb_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2954\u001b[0m   (graph_function,\n\u001b[0;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1856\u001b[0m     args,\n\u001b[0;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1858\u001b[0m     executing_eagerly)\n\u001b[0;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mF:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=2000, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "214474ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "97f00f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b35f00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[298,   0],\n",
       "        [  0,   6]],\n",
       "\n",
       "       [[295,   4],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[298,   2],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[300,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[296,   0],\n",
       "        [  0,   8]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[297,   2],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[298,   0],\n",
       "        [  0,   6]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[299,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  1,   2]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[296,   0],\n",
       "        [  2,   6]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[298,   0],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[298,   2],\n",
       "        [  2,   2]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[298,   0],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[301,   2],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[297,   0],\n",
       "        [  0,   7]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  2,   3]],\n",
       "\n",
       "       [[298,   2],\n",
       "        [  2,   2]],\n",
       "\n",
       "       [[301,   1],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  2,   2]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[297,   1],\n",
       "        [  0,   6]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[298,   3],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[300,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[300,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  2,   1]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  4,   0]],\n",
       "\n",
       "       [[299,   4],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  2,   3]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[299,   2],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  2,   3]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[298,   0],\n",
       "        [  2,   4]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  2,   0]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[302,   0],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[300,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[295,   0],\n",
       "        [  6,   3]],\n",
       "\n",
       "       [[300,   2],\n",
       "        [  0,   2]],\n",
       "\n",
       "       [[300,   1],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[300,   0],\n",
       "        [  0,   4]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[301,   2],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[301,   0],\n",
       "        [  0,   3]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[299,   0],\n",
       "        [  0,   5]],\n",
       "\n",
       "       [[303,   0],\n",
       "        [  0,   1]],\n",
       "\n",
       "       [[299,   1],\n",
       "        [  2,   2]]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "\n",
    "multilabel_confusion_matrix(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c288b2b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8782894736842105"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a3cb964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "<class 'mediapipe.python.solution_base.SolutionOutputs'>\n",
      "in user code:\n",
      "\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1801, in predict_function  *\n",
      "        return step_function(self, iterator)\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1790, in step_function  **\n",
      "        outputs = model.distribute_strategy.run(run_step, args=(data,))\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1783, in run_step  **\n",
      "        outputs = model.predict_step(data)\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\training.py\", line 1751, in predict_step\n",
      "        return self(x, training=False)\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n",
      "        raise e.with_traceback(filtered_tb) from None\n",
      "    File \"F:\\Academics\\sem7\\handPoseEstimation\\env\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 264, in assert_input_compatibility\n",
      "        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n",
      "\n",
      "    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 18, 258), found shape=(None, 23, 1662)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. New detection variables\n",
    "sequence = []\n",
    "sentence = []\n",
    "predictions = []\n",
    "threshold = 0.9\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set mediapipe model \n",
    "try:\n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        count = 0\n",
    "        while cap.isOpened():\n",
    "            done = False\n",
    "            # Read feed\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Make detections\n",
    "            image, results = mp_detection(frame, holistic)\n",
    "            print(results)\n",
    "\n",
    "            # Draw landmarks\n",
    "            draw_landmarks(image, results)\n",
    "\n",
    "            # 2. Prediction logic\n",
    "            keypoints = extract_keypoints(results)\n",
    "    #         sequence.insert(0,keypoints)\n",
    "    #         sequence = sequence[:30]\n",
    "            sequence.append(keypoints)\n",
    "            sequence = sequence[-23:]\n",
    "            if len(sequence) == 23:\n",
    "                res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "                print(words[np.argmax(res)])\n",
    "                predictions.append(np.argmax(res))\n",
    "\n",
    "\n",
    "            #3. Viz logic\n",
    "                if np.unique(predictions[-20:])[0]==np.argmax(res): \n",
    "                    if res[np.argmax(res)] > threshold: \n",
    "\n",
    "                        if len(sentence) > 0: \n",
    "                            if words[np.argmax(res)] != sentence[-1]:\n",
    "                                sentence.append(words[np.argmax(res)])\n",
    "                        else:\n",
    "                            sentence.append(words[np.argmax(res)])\n",
    "\n",
    "                if len(sentence) > 5: \n",
    "                    sentence = sentence[-5:]\n",
    "\n",
    "                # Viz probabilities\n",
    "                image = prob_viz(res, words, image, colors)\n",
    "                    \n",
    "            cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "            cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Show to screen\n",
    "            cv2.imshow('OpenCV Feed', image)\n",
    "\n",
    "            # Break gracefully\n",
    "            if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                break\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
